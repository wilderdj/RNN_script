{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 62 characters\n"
     ]
    }
   ],
   "source": [
    "textfile = (\"C:/Users/wilde/Downloads/Python_Certification/house_script.txt\")\n",
    "data = open(textfile, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(textfile)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rebecca riding bus and then running into a school. Meets up with Melanie] Melanie: Why are you late? Rebecca: You’re not going to like the answer. Melanie: I already know the answer. Rebecca: I missed the bus. Melanie: I don’t doubt it, no bus stops\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(data[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(data))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(data):\n",
    "  return np.array([char2idx[c] for c in data])\n",
    "\n",
    "text_as_int = text_to_int(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Rebecca riding\n",
      "Encoded: [47 40 53 50 53 51 51 49  0 66 57 52 57 62 55]\n"
     ]
    }
   ],
   "source": [
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", data[:15])\n",
    "print(\"Encoded:\", text_to_int(data[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rebecca riding\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(data)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "[Rebecca riding bus and then running into a school. Meets up with Melanie] Melanie: Why are you late\n",
      "\n",
      "OUTPUT\n",
      "Rebecca riding bus and then running into a school. Meets up with Melanie] Melanie: Why are you late?\n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      " Rebecca: You’re not going to like the answer. Melanie: I already know the answer. Rebecca: I missed\n",
      "\n",
      "OUTPUT\n",
      "Rebecca: You’re not going to like the answer. Melanie: I already know the answer. Rebecca: I missed \n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print(\"\\n\\nEXAMPLE\\n\")\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"\\nOUTPUT\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data1 = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           20736     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 81)            83025     \n",
      "=================================================================\n",
      "Total params: 5,350,737\n",
      "Trainable params: 5,350,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 81) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data1.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-2.23537069e-03  4.67018457e-03 -4.21498576e-03 ... -2.91747274e-04\n",
      "    8.06665979e-04  2.68795236e-04]\n",
      "  [ 1.74955465e-04  2.34455056e-03 -4.66846512e-04 ... -4.46083210e-03\n",
      "    2.12008599e-04 -3.38360062e-03]\n",
      "  [ 4.63488884e-03  6.24307198e-04 -1.91560714e-04 ...  5.23102842e-03\n",
      "   -4.48480132e-05 -3.85127449e-03]\n",
      "  ...\n",
      "  [ 3.72150471e-03 -7.94675667e-03  1.52940745e-03 ... -3.52413696e-03\n",
      "   -3.53572192e-03  6.90409914e-04]\n",
      "  [ 2.14113574e-03 -7.21795764e-03  9.16266628e-03 ... -4.24564490e-03\n",
      "   -1.02019282e-02  4.81819734e-04]\n",
      "  [ 5.73389884e-03  7.41083757e-04 -2.34509353e-04 ... -2.30242871e-03\n",
      "   -1.28783621e-02  6.38580509e-03]]\n",
      "\n",
      " [[ 3.74242919e-03 -4.16364055e-03  2.26221071e-03 ... -4.85948432e-04\n",
      "    9.38715413e-04  2.32780236e-03]\n",
      "  [ 4.41983901e-03 -1.00536030e-02  1.53552811e-03 ... -6.79928996e-03\n",
      "    5.62956417e-03 -1.67085533e-03]\n",
      "  [ 1.89662655e-03 -6.68851426e-03 -4.71756933e-03 ... -3.83423106e-03\n",
      "    5.15405554e-03 -1.22145680e-03]\n",
      "  ...\n",
      "  [ 1.42225623e-02  6.73829718e-03 -7.48930871e-03 ...  2.74430960e-04\n",
      "   -1.68165639e-02  6.14213105e-03]\n",
      "  [ 1.26882810e-02  2.53478554e-03 -3.75544513e-03 ... -5.37003530e-03\n",
      "   -1.27594750e-02  9.11599491e-05]\n",
      "  [ 4.96107666e-03 -5.36013721e-03  1.22431153e-03 ... -1.49055687e-03\n",
      "   -6.70578331e-03  2.09250161e-03]]\n",
      "\n",
      " [[-4.84494842e-04 -8.08320474e-05  7.61384284e-03 ...  1.33579248e-04\n",
      "   -6.41409121e-03 -5.62961097e-04]\n",
      "  [-2.72662565e-03  4.05752240e-03  9.30835959e-04 ...  3.28163151e-05\n",
      "   -3.01096402e-03  2.09571590e-04]\n",
      "  [ 1.27661333e-04 -1.99827319e-03 -2.95708235e-03 ... -4.96262964e-03\n",
      "   -2.95649306e-03 -3.17382882e-03]\n",
      "  ...\n",
      "  [ 6.93919742e-03  4.99046221e-03 -6.01474941e-03 ...  6.61524478e-03\n",
      "   -5.77632198e-03  7.18629546e-03]\n",
      "  [ 1.33321183e-02  3.33911157e-04 -2.06869654e-03 ...  4.83490247e-03\n",
      "   -3.12011689e-03  5.71327750e-04]\n",
      "  [ 1.24086561e-02 -3.48792807e-03 -4.84633679e-03 ...  9.54018161e-03\n",
      "   -4.98473877e-03  4.24203114e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.77254842e-03 -5.90319280e-03  5.92638040e-04 ... -5.78318024e-03\n",
      "    4.70374245e-03 -2.35960633e-03]\n",
      "  [ 1.10506651e-03 -4.24110331e-03  6.88631088e-03 ... -3.60942516e-03\n",
      "   -3.05235106e-03 -2.48306245e-03]\n",
      "  [-2.39395048e-03 -5.30576985e-03  2.67904019e-03 ... -5.03896084e-03\n",
      "   -4.71121166e-06 -6.56004588e-04]\n",
      "  ...\n",
      "  [ 6.87432662e-03 -3.31823085e-03  4.21261881e-03 ... -7.80552917e-04\n",
      "   -3.29858181e-03  4.87863505e-03]\n",
      "  [ 2.41540093e-03 -9.42638377e-04  8.44193622e-04 ...  1.40974915e-03\n",
      "   -5.11835702e-03  9.66789317e-04]\n",
      "  [ 4.70621418e-03 -4.57527256e-03  2.20403913e-03 ... -1.14635308e-03\n",
      "   -3.52985505e-03  2.17307126e-03]]\n",
      "\n",
      " [[ 7.80334603e-03 -2.22527399e-03  3.14926868e-03 ...  1.04392669e-03\n",
      "    1.22031383e-03 -4.00955230e-03]\n",
      "  [ 9.72193480e-03 -6.73419423e-03  4.47471067e-03 ... -9.44032799e-05\n",
      "    1.37604959e-03 -3.09343915e-04]\n",
      "  [ 6.82976563e-03 -6.78161485e-03  1.03577618e-02 ... -9.33401752e-04\n",
      "   -5.49351145e-03 -1.59687665e-03]\n",
      "  ...\n",
      "  [ 9.90295224e-03 -1.17199179e-02  2.10666261e-03 ...  1.29705016e-02\n",
      "   -6.24601310e-03 -1.00144255e-03]\n",
      "  [ 9.68880206e-03 -1.27186039e-02  1.30457769e-03 ...  1.28260814e-02\n",
      "   -1.55571918e-03  1.65049359e-03]\n",
      "  [ 1.09357927e-02 -1.26536051e-02 -3.29976738e-03 ...  1.61025263e-02\n",
      "   -3.64626897e-03  5.31769707e-04]]\n",
      "\n",
      " [[ 1.77254842e-03 -5.90319280e-03  5.92638040e-04 ... -5.78318024e-03\n",
      "    4.70374245e-03 -2.35960633e-03]\n",
      "  [ 1.03013944e-02 -7.18616415e-04  1.09328527e-03 ... -9.04311775e-04\n",
      "    1.01019945e-02 -9.57637373e-03]\n",
      "  [ 8.65116529e-03  7.29218300e-04  7.39042368e-03 ... -5.57171181e-04\n",
      "    1.09045405e-03 -8.44137743e-03]\n",
      "  ...\n",
      "  [ 2.89354566e-03 -9.82572697e-03  1.24007724e-02 ... -1.14135444e-03\n",
      "   -1.92216923e-03  2.82077026e-03]\n",
      "  [ 5.66181121e-03 -6.01497712e-03  6.40001614e-03 ... -9.34430049e-04\n",
      "   -4.49023442e-03  5.45861665e-03]\n",
      "  [ 3.96794127e-03 -4.77089826e-03  1.23096034e-02 ... -1.59614813e-03\n",
      "   -1.11098848e-02  3.67464917e-03]]], shape=(64, 100, 81), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-2.2353707e-03  4.6701846e-03 -4.2149858e-03 ... -2.9174727e-04\n",
      "   8.0666598e-04  2.6879524e-04]\n",
      " [ 1.7495546e-04  2.3445506e-03 -4.6684651e-04 ... -4.4608321e-03\n",
      "   2.1200860e-04 -3.3836006e-03]\n",
      " [ 4.6348888e-03  6.2430720e-04 -1.9156071e-04 ...  5.2310284e-03\n",
      "  -4.4848013e-05 -3.8512745e-03]\n",
      " ...\n",
      " [ 3.7215047e-03 -7.9467567e-03  1.5294075e-03 ... -3.5241370e-03\n",
      "  -3.5357219e-03  6.9040991e-04]\n",
      " [ 2.1411357e-03 -7.2179576e-03  9.1626663e-03 ... -4.2456449e-03\n",
      "  -1.0201928e-02  4.8181973e-04]\n",
      " [ 5.7338988e-03  7.4108376e-04 -2.3450935e-04 ... -2.3024287e-03\n",
      "  -1.2878362e-02  6.3858051e-03]], shape=(100, 81), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "tf.Tensor(\n",
      "[-2.2353707e-03  4.6701846e-03 -4.2149858e-03 -1.6093720e-05\n",
      " -4.2813428e-04 -4.9937870e-03 -2.8394896e-03 -4.9471506e-03\n",
      " -2.8028712e-04 -9.6414331e-04 -3.9227563e-03 -2.5933625e-03\n",
      " -1.1733677e-03 -1.3221469e-03 -1.6474854e-03 -9.7166549e-04\n",
      "  1.3041871e-03 -1.4191106e-04  1.4095142e-04  3.5947065e-03\n",
      " -2.9467803e-04  2.5010030e-03 -1.1661400e-03  7.8250244e-03\n",
      " -3.3105249e-03  3.0399724e-03  1.6730226e-04 -1.6808566e-03\n",
      "  5.0318446e-03 -3.3447257e-04 -1.9083279e-03 -5.0799213e-03\n",
      "  3.2489426e-03  2.8114188e-03 -6.2018437e-03  2.2074319e-03\n",
      " -3.8723093e-03  3.4873085e-03  1.1366002e-03 -2.1958142e-03\n",
      " -1.9135608e-03  4.6272529e-03  2.4673515e-03 -1.1462183e-03\n",
      " -1.3704087e-03  1.6071012e-03 -7.2384824e-04 -1.7602514e-03\n",
      " -8.9801662e-03 -7.6061510e-03 -1.5912405e-03 -4.6333945e-03\n",
      "  2.0010490e-03 -1.4097665e-03  1.5476961e-03 -1.3545397e-03\n",
      "  1.0124899e-03  9.4640302e-04 -2.4461865e-03  2.0117261e-03\n",
      " -1.0872702e-04 -1.1217548e-03  2.6374483e-03  5.6375028e-03\n",
      " -4.2691338e-03 -6.6609923e-03  4.9884757e-03  7.1050385e-03\n",
      " -4.0556793e-03  1.2587343e-03  9.0317114e-04 -3.2986500e-03\n",
      "  7.0362394e-03 -2.2945267e-03  4.0278686e-03  5.4141688e-03\n",
      " -2.2253932e-03  4.3367653e-04 -2.9174727e-04  8.0666598e-04\n",
      "  2.6879524e-04], shape=(81,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally we'll look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw1;aeqP][H;}5}Uj9y?Cnx]qx}CFT?e,gEE,5’zh-T;1cUCLxmr*”dIO‘$J$%NTi.R:Tdh?DV8g%”W2L;,o3F6WVv5wTU7w($7]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 105s 20s/step - loss: 4.1889\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 323s 55s/step - loss: 3.3319\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 325s 51s/step - loss: 3.2016\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 240s 41s/step - loss: 3.1611\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 241s 41s/step - loss: 3.1312\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 40s 5s/step - loss: 3.1047\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 249s 49s/step - loss: 3.0513\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 241s 40s/step - loss: 2.9880\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 238s 40s/step - loss: 2.8899\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 240s 41s/step - loss: 2.7568\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 249s 42s/step - loss: 2.6257\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 255s 43s/step - loss: 2.5295\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 260s 45s/step - loss: 2.4600\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 243s 41s/step - loss: 2.4095\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 246s 42s/step - loss: 2.3737\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 250s 41s/step - loss: 2.3162\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 246s 42s/step - loss: 2.2786\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 687s 126s/step - loss: 2.2328\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 127s 24s/step - loss: 2.1932\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 312s 53s/step - loss: 2.1703\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 280s 47s/step - loss: 2.1371\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 99s 11s/step - loss: 2.0920\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 226s 44s/step - loss: 2.0599\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 29s 5s/step - loss: 2.0335\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 284s 51s/step - loss: 1.9833\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 295s 49s/step - loss: 1.9689\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 288s 49s/step - loss: 1.9307\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 124s 16s/step - loss: 1.9037\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 91s 17s/step - loss: 1.8630\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 90s 17s/step - loss: 1.8448\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 98s 19s/step - loss: 1.8056\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 71s 6s/step - loss: 1.7767\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 243s 48s/step - loss: 1.7492\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 202s 32s/step - loss: 1.7225\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 102s 19s/step - loss: 1.6770\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 319s 54s/step - loss: 1.6663\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 332s 52s/step - loss: 1.6333\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 105s 20s/step - loss: 1.6065\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 249s 39s/step - loss: 1.5822\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 29s 5s/step - loss: 1.5501\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 265s 47s/step - loss: 1.5161\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 75s 6s/step - loss: 1.4786\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 290s 51s/step - loss: 1.4501\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 299s 50s/step - loss: 1.4188\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 98s 10s/step - loss: 1.3742\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 179s 35s/step - loss: 1.3507\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 147s 28s/step - loss: 1.3030\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 198s 31s/step - loss: 1.2649\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 96s 18s/step - loss: 1.2477\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 257s 43s/step - loss: 1.2013\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data1, epochs=50, callbacks=[checkpoint_callback]) #fits the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # creating checkpoints \n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type a starting string:  House:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House: You have! House: Bven a wormill. soured I canese twick hove has wakes it’s hine! House: She’s not batinad, agy the maspertion the erpapitt die.. [Cut oo a tum” ofen the frodiman to fif jom. gitter is you’re gring and atages. Foreman: I’m pert if bett digntir. Cameron: Yeah, he’s about mide some gonnateres] [Cut to cofl. Dreatiny. House: There’s not liago... House: No, brobation. [House just like you vesy and gotse think I fach the is andengerondan a mory mind. Wimson: Cuddy: Orase you’ve do. It’s o tuld swet if the exis touss to gurarie am probame. Ferek wese goond no. [Cut to House alleigots and peasepat.]] House: No, you mayt and megut stiof staiclled of the insented cause thin must evon into to hen ag, iln’t like you candias into betso hoppliwe day. “Couthing! House liging to mase anyt\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \") # Starting string for the model\n",
    "print(generate_text(model, inp)) # prints the predicted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
